---
title: "Testing -Text classification"
layout: post
date: 2018-02-16 15:48
image: /assets/images/markdown.jpg
headerImage: false
tag:
- markdown
- components
- extra
category: blog
author: jamesfoster
description: Markdown summary with different options
# jemoji: '<img class="emoji" title=":ramen:" alt=":ramen:" src="https://assets.github.com/images/icons/emoji/unicode/1f35c.png" height="20" width="20" align="absmiddle">'
---


```python
### get the data
```


```python
messages = [line.rstrip() for line in open('./sms/SMSSpamCollection')]
```


```python
print(len(messages))
```

    5574



```python
## letrs check the DS
```


```python
type(messages)
```




    list




```python
messages[10]
```




    "ham\tI'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today."




```python
### hence message te is not a spam
```


```python
##lets print  first ten and give it a numver
```


```python
for message_no, message in enumerate(messages[:10]):
    print(message_no, message)
    print('\n')
```

    0 ham	Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...
    
    
    1 ham	Ok lar... Joking wif u oni...
    
    
    2 spam	Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's
    
    
    3 ham	U dun say so early hor... U c already then say...
    
    
    4 ham	Nah I don't think he goes to usf, he lives around here though
    
    
    5 spam	FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv
    
    
    6 ham	Even my brother is not like to speak with me. They treat me like aids patent.
    
    
    7 ham	As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune
    
    
    8 spam	WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.
    
    
    9 spam	Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030
    
    



```python
### u notice it print the label and the data then we use enumerate to give it a number
```


```python
%%html
<h1>Now we need pands to make life easier for us </h1>
```


<h1>Now we need pands to make life easier for us </h1>



```python
import pandas as pd
```


```python
messages = pd.read_csv('./sms/SMSSpamCollection', sep='\t',
                           names=["label", "message"])
```


```python
##### notice above that we the file is \t separeted so we gotta separate it
### default for the function isnacyaually comma
```


```python
### get first five data
```


```python
messages.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives aro...</td>
    </tr>
  </tbody>
</table>
</div>




```python
%%html
<h2>Part 2: Basic Exploratory Data Analysis¶
</h2>
```


<h2>Part 2: Basic Exploratory Data Analysis¶
</h2>



```python
messages.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5572</td>
      <td>5572</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>2</td>
      <td>5169</td>
    </tr>
    <tr>
      <th>top</th>
      <td>ham</td>
      <td>Sorry, I'll call later</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>4825</td>
      <td>30</td>
    </tr>
  </tbody>
</table>
</div>




```python
%%html
<h4>Let's use groupby to use describe by label, this way we can begin to think about the features that separate ham and spam</h4>
```


<h4>Let's use groupby to use describe by label, this way we can begin to think about the features that separate ham and spam</h4>



```python
## note that groupby is use to regroup the dataset
```


```python
messages.groupby('label').describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="4" halign="left">message</th>
    </tr>
    <tr>
      <th></th>
      <th>count</th>
      <th>unique</th>
      <th>top</th>
      <th>freq</th>
    </tr>
    <tr>
      <th>label</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ham</th>
      <td>4825</td>
      <td>4516</td>
      <td>Sorry, I'll call later</td>
      <td>30</td>
    </tr>
    <tr>
      <th>spam</th>
      <td>747</td>
      <td>653</td>
      <td>Please call our customer service representativ...</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




```python
%%html
<h4>get the length of each meswsage </h4>
```


<h4>get the length of each meswsage </h4>



```python
##### notice we use len function... len simply take datapoint and work on it
```


```python
messages['length'] = messages['message'].apply(len)
messages.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>message</th>
      <th>length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only ...</td>
      <td>111</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
      <td>29</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
      <td>155</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
      <td>49</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives aro...</td>
      <td>61</td>
    </tr>
  </tbody>
</table>
</div>




```python
%%html
<h4> a lil bit of visualization</h4>
```


<h4> a lil bit of visualization</h4>



```python
import matplotlib.pyplot as plt
import seaborn as sns
```


```python
####### lets visualize the length
```


```python
messages['length'].plot(bins=50, kind='hist')
```




    <matplotlib.axes._subplots.AxesSubplot at 0xab06532c>




```python
%matplotlib inline
```


```python
messages['length'].plot(bins=50, kind='hist',figsize=(15,6))
```




    <matplotlib.axes._subplots.AxesSubplot at 0xab05906c>



![png](/assets/images/NLP_29_1.png)



```python
messages['length'].plot(bins=50, kind='hist')
```




    <matplotlib.axes._subplots.AxesSubplot at 0xa9dd4c8c>




![png](/assets/images/NLP_30_1.png)



```python
messages['length'].plot(bins=20, kind='hist')
```




    <matplotlib.axes._subplots.AxesSubplot at 0xa9d0f36c>




![png](/assets/images/NLP_31_1.png)



```python
messages['length'].plot(bins=80, kind='hist')
```




    <matplotlib.axes._subplots.AxesSubplot at 0xa9c1864c>




![png](/assets/images/NLP_32_1.png)



```python
############ lets get summary stst based on length
```


```python
messages['length'].describe()
```




    count    5572.000000
    mean       80.489950
    std        59.942907
    min         2.000000
    25%        36.000000
    50%        62.000000
    75%       122.000000
    max       910.000000
    Name: length, dtype: float64




```python
ans = messages['length'].unique()
```


```python
len(ans)
```




    274




```python
### as u can see bar chart doesw not make for length
```


```python
### so we use interval data
```


```python
#### max len for the meswsage is 910...thas crazy
```


```python
#### lets find it out
```


```python
len910 = messages[messages['length'] == 910]
```


```python
len(len910)
```




    1




```python

```


```python
len910
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>message</th>
      <th>length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1085</th>
      <td>ham</td>
      <td>For me the love should start with attraction.i...</td>
      <td>910</td>
    </tr>
  </tbody>
</table>
</div>




```python
len910['message']
```




    1085    For me the love should start with attraction.i...
    Name: message, dtype: object




```python
messages[messages['length'] == 910]['message'].iloc[0]
```




    "For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later.."




```python
len910['message'].iloc[0]
```




    "For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later.."




```python
aa = len910['message']
```


```python
type(aa)
```




    pandas.core.series.Series




```python
aa
```




    1085    For me the love should start with attraction.i...
    Name: message, dtype: object




```python
len(aa)
```




    1




```python
aa.iloc[0]
```




    "For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later.."




```python
#
```


```python
#### lets plot the length based on label ...this mean we gonna print two grp
```


```python

messages.hist(column='length', by='label', bins=50,figsize=(10,4))
```




    array([<matplotlib.axes._subplots.AxesSubplot object at 0xa96ffe4c>,
           <matplotlib.axes._subplots.AxesSubplot object at 0xa96e69ec>],
          dtype=object)




![png](assets/images/NLP_55_1.png)



```python
### the above EDA show spam message tends to have more characters
```


```python
%%html
<h2>Part 3: Text pre-processing</h2>
```


<h2>Part 3: Text pre-processing</h2>



```python
### our text need to be converted to numeber for ML
```


```python
import string

mess = 'Sample message! Notice: it has punctuation.'

# Check characters to see if they are in punctuation
nopunc = [char for char in mess if char not in string.punctuation] ### note here

# Join the characters again to form the string.
nopunc = ''.join(nopunc)
```


```python
nopunc
```




    'Sample message Notice it has punctuation'




```python
from nltk.corpus import stopwords
stopwords.words('english')[0:10] # Show some stop words
```




    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]




```python
nopunc.split()
```




    ['Sample', 'message', 'Notice', 'it', 'has', 'punctuation']




```python
#### now we will use the above to break our dataset from pandas dataframe
```


```python
def text_process(mess):
    """
    Takes in a string of text, then performs the following:
    1. Remove all punctuation
    2. Remove all stopwords
    3. Returns a list of the cleaned text
    """
    # Check characters to see if they are in punctuation
    nopunc = [char for char in mess if char not in string.punctuation]

    # Join the characters again to form the string.
    nopunc = ''.join(nopunc)
    
    # Now just remove any stopwords
    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]
```


```python
messages.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>message</th>
      <th>length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only ...</td>
      <td>111</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
      <td>29</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
      <td>155</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
      <td>49</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives aro...</td>
      <td>61</td>
    </tr>
  </tbody>
</table>
</div>




```python
######### lets test it..remeber like the abve len

### each value of message will be handle
```


```python
# Check to make sure its working
messages['message'].head(5).apply(text_process)
```




    0    [Go, jurong, point, crazy, Available, bugis, n...
    1                       [Ok, lar, Joking, wif, u, oni]
    2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...
    3        [U, dun, say, early, hor, U, c, already, say]
    4    [Nah, dont, think, goes, usf, lives, around, t...
    Name: message, dtype: object




```python
### it working
```


```python
%%html
<h4>Certainly we can do more nomalization</>
<small>like stemming, lemmatization</small>
<smmal>But social media make it so hard...hence we gonna come up
with a way of solving this like my previous work</small></h4>
```


<h4>Certainly we can do more nomalization</>
<small>like stemming, lemmatization</small>
<smmal>But social media make it so hard...hence we gonna come up
with a way of solving this like my previous work</small></h4>



```python
%%html
<h2>Part 4: Vectorization</h2>
```


<h2>Part 4: Vectorization</h2>



```python
## we have the function that can tokenize the mesage but we have to vectorise it
#### so as to let ML work on it
```


```python
%%html
<h2>WE do vectorization 3 steps</h2>
<ul>
<li>bag of word</li>
<li>then use tf-idf</li>
</ul>
```


<h2>WE do vectorization 3 steps</h2>
<ul>
<li>bag of word</li>
<li>then use tf-idf</li>
</ul>



```python
from sklearn.feature_extraction.text import CountVectorizer
```


```python
bow_transformer = CountVectorizer(analyzer=text_process) ### bets way is to use it this way
```


```python
bow_transformer = bow_transformer.fit(messages['message'])
```


```python
###### i think is hould be usng th=is above method fot vectorization and bag of word


#### much more better than what i used to do before
```


```python
############ lets see the number of features we extract
```


```python
# Print total number of vocab words
print(len(bow_transformer.vocabulary_))
```

    11425



```python
#### wow thas simply massive
```


```python
%%html
<h3>Let's take one text message and get its bag-of-words counts as a vector, putting to use our new bow_transformer:</h3>
```


<h3>Let's take one text message and get its bag-of-words counts as a vector, putting to use our new bow_transformer:</h3>



```python

message4 = messages['message'][3]
print(message4)
```

    U dun say so early hor... U c already then say...



```python
### Now let's see its vector representation:
```


```python
bow4 = bow_transformer.transform([message4])
print(bow4)
print(bow4.shape)

```

      (0, 4068)	2
      (0, 4629)	1
      (0, 5261)	1
      (0, 6204)	1
      (0, 6222)	1
      (0, 7186)	1
      (0, 9554)	2
    (1, 11425)



```python
type(bow_transformer)
```




    sklearn.feature_extraction.text.CountVectorizer




```python
%%html
<h4>This means that there are seven unique words in message number 4 (after removing common stop words). Two of them appear twice, the rest only once. Let's go ahead and check and confirm which ones appear twice:</h4>
```


<h4>This means that there are seven unique words in message number 4 (after removing common stop words). Two of them appear twice, the rest only once. Let's go ahead and check and confirm which ones appear twice:</h4>



```python
print(bow_transformer.get_feature_names()[4073])
print(bow_transformer.get_feature_names()[9570])
```

    UIN
    schedule



```python
%%html
<h4>Now we can use .transform on our Bag-of-Words (bow) transformed object and transform the entire DataFrame of messages. Let's go ahead and check out how the bag-of-words counts for the entire SMS corpus is a large, sparse matrix:</h4>
```


<h4>Now we can use .transform on our Bag-of-Words (bow) transformed object and transform the entire DataFrame of messages. Let's go ahead and check out how the bag-of-words counts for the entire SMS corpus is a large, sparse matrix:</h4>



```python
messages_bow = bow_transformer.transform(messages['message'])
print('Shape of Sparse Matrix: ', messages_bow.shape)
print('Amount of Non-Zero occurences: ', messages_bow.nnz)
print('sparsity: %.2f%%' % (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1])))
```

    Shape of Sparse Matrix:  (5572, 11425)
    Amount of Non-Zero occurences:  50548
    sparsity: 0.08%



```python
### the above should be self explanatory
```


```python
%%html
<h4>
After the counting, the term weighting and normalization can be done with TF-IDF, using scikit-learn's TfidfTransformer.
</h4>
```


<h4>
After the counting, the term weighting and normalization can be done with TF-IDF, using scikit-learn's TfidfTransformer.
</h4>



```python
%%html
<p><h2>So what is TF-IDF?</h3>
TF-IDF stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.

One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.

Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.

<h4>TF:</h4> Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:

<h4>TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).
</h4>
<h4>IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:
</h4>
IDF(t) = log_e(Total number of documents / Number of documents with term t in it).

See below for a simple example.

Example:

Consider a document containing 100 words wherein the word cat appears 3 times.

The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.

</p>
```


<p><h2>So what is TF-IDF?</h3>
TF-IDF stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.

One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.

Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.

<h4>TF:</h4> Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:

<h4>TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).
</h4>
<h4>IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:
</h4>
IDF(t) = log_e(Total number of documents / Number of documents with term t in it).

See below for a simple example.

Example:

Consider a document containing 100 words wherein the word cat appears 3 times.

The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.

</p>



```python
######################## Let's go ahead and see how we can do this in SciKit Learn:
```


```python
from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer().fit(messages_bow)
tfidf4 = tfidf_transformer.transform(bow4) ### we just test it on this asw well but above
#### is the real deal......we use a trained tf-idf to transorfm bow4
print(tfidf4)


### note that we use Bag of word to fit tf-idf
```

      (0, 9554)	0.5385626262927565
      (0, 7186)	0.43893656533798575
      (0, 6222)	0.3187216892949149
      (0, 6204)	0.2995379972369742
      (0, 5261)	0.2972995740586873
      (0, 4629)	0.2661980190608719
      (0, 4068)	0.4083258993338407


    /opt/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):



```python
####


#### We'll go ahead and check what is the IDF (inverse document frequency) of the word "u"? Of word "university"?
```


```python
print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])
print(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])
```

    3.2800524267409408
    8.527076498901426



```python
%%html
<H4>nOTICE THAT we just fit the tf-idf we still need to tranform</h3>
```


<H4>nOTICE THAT we just fit the tf-idf we still need to tranform</h3>



```python
messages_tfidf = tfidf_transformer.transform(messages_bow)
print(messages_tfidf.shape)
```

    (5572, 11425)


    /opt/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):



```python
%%html
<h1>Part 5: Training a model</h1>
```


<h1>Part 5: Training a model</h1>



```python
from sklearn.naive_bayes import MultinomialNB
spam_detect_model = MultinomialNB().fit(messages_tfidf, messages['label'])
```


```python
%%html
<p>
Let's try classifying our single random message and checking how we do:</p>
```


<p>
Let's try classifying our single random message and checking how we do:</p>



```python
print('predicted:', spam_detect_model.predict(tfidf4)[0])
print('expected:', messages.label[3])
```

    predicted: ham
    expected: ham



```python
## the prediction is correct
```


```python
%%html
<h4>Fantastic! We've developed a model that can attempt to predict spam vs ham classification</h4>
```


<h4>Fantastic! We've developed a model that can attempt to predict spam vs ham classification</h4>



```python
%%html
<h1>Part 6: Model Evaluation</h1>
```


<h1>Part 6: Model Evaluation</h1>



```python
all_predictions = spam_detect_model.predict(messages_tfidf) ## dnt be confuesd here
print(all_predictions)
```

    ['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']



```python
from sklearn.metrics import classification_report
print(classification_report(messages['label'], all_predictions))
```

                 precision    recall  f1-score   support
    
            ham       0.98      1.00      0.99      4825
           spam       1.00      0.85      0.92       747
    
    avg / total       0.98      0.98      0.98      5572
    



```python
##### Hey remember that we just use the data we use to train also tom test


#### u dont do this in real life because u will not know th e true


### predictive power of ur model
```


```python
%%html
<h4>
A proper way is to split the data into a training/test set, where the model only ever sees the training data during its model fitting and parameter tuning. The test data is never used in any way. This is then our final evaluation on test data is representative of true predictive performance.
</h4>
```


<h4>
A proper way is to split the data into a training/test set, where the model only ever sees the training data during its model fitting and parameter tuning. The test data is never used in any way. This is then our final evaluation on test data is representative of true predictive performance.
</h4>



```python
from sklearn.cross_validation import train_test_split

msg_train, msg_test, label_train, label_test = \
train_test_split(messages['message'], messages['label'], test_size=0.2)

print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))
```

    4457 1115 5572


    /opt/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
      "This module will be removed in 0.20.", DeprecationWarning)



```python
%%html
<h1>Part 7: Creating a Data Pipeline</h1>
```


<h1>Part 7: Creating a Data Pipeline</h1>



```python
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts
    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier
])
```


```python
%%html
<p>Now we can directly pass message text data and the pipeline will do our pre-processing for us! We can treat it as a model/estimator API:</p>
```


<p>Now we can directly pass message text data and the pipeline will do our pre-processing for us! We can treat it as a model/estimator API:</p>



```python
pipeline.fit(msg_train,label_train)
```

    /opt/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):





    Pipeline(steps=[('bow', CountVectorizer(analyzer=<function text_process at 0xa5b4d0bc>, binary=False,
            decode_error='strict', dtype=<class 'numpy.int64'>,
            encoding='utf-8', input='content', lowercase=True, max_df=1.0,
            max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None,
    ...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])




```python
predictions = pipeline.predict(msg_test)
```

    /opt/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):



```python
print(classification_report(predictions,label_test))
```

                 precision    recall  f1-score   support
    
            ham       1.00      0.96      0.98       999
           spam       0.77      1.00      0.87       116
    
    avg / total       0.98      0.97      0.97      1115
    



```python
%%html
<h1 style="color:red">
Building Model Popelin for is like a amagic..just make sure u provide the appropritae
analyzer...it is amazin!!!</h1>
```


<h1 style="color:red">
Building Model Popelin for is like a amagic..just make sure u provide the appropritae
analyzer...it is amazin!!!</h1>



```python

```
